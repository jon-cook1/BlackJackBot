{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    # learn_r -> learning rate      exp_r -> experimenting rate (how often to randomly choose)\n",
    "    def __init__(self, learn_r=0.1, exp_r=0.2, train=True):  \n",
    "\n",
    "        self.player_Q_Values = {}  \n",
    "        \n",
    "        # key: [(player_val, up_card, ace)][action] = value\n",
    "        # initialise Q values | (12-21) x (1-10) x (True, False) x (1, 0) 400 in total\n",
    "            # This essentially represents each possible action in the game. Each combination of\n",
    "                # player sum (hard and soft)/ dealer up card with 1 and 0 to represent the choice for that combination\n",
    "        \n",
    "        for i in range(12, 22):\n",
    "            for j in range(1, 11):\n",
    "                for k in [True, False]:\n",
    "                    self.player_Q_Values[(i, j, k)] = {}\n",
    "                    for a in [1, 0]:\n",
    "                        if i == 21 and a == 0:\n",
    "                            self.player_Q_Values[(i, j, k)][a] = 1\n",
    "                        else:\n",
    "                            self.player_Q_Values[(i, j, k)][a] = 0\n",
    "        \n",
    "        \n",
    "        self.state = (0, 0, False)  # default state\n",
    "        self.actions = [1, 0]  # 1 -> hit, 0 -> stand\n",
    "        self.player_state_action = []\n",
    "        self.end = False\n",
    "        self.learn_r = learn_r\n",
    "        self.exp_r = exp_r\n",
    "        self.training = train\n",
    "\n",
    "    # Same logic here as in MC\n",
    "    @staticmethod\n",
    "    def giveCard():\n",
    "        cards = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "        return np.random.choice(cards)\n",
    "    \n",
    "    # Used to allow auto win if 21 is pulled off deal\n",
    "    def deal2cards(self, show=False):\n",
    "        cards = [self.giveCard(), self.giveCard()]\n",
    "        \n",
    "        val = sum(cards)\n",
    "        if 1 in cards:\n",
    "            val += 10\n",
    "            ace = True\n",
    "        else:\n",
    "            ace = False\n",
    "\n",
    "        if show:\n",
    "            return val, ace, cards[0]\n",
    "        else:\n",
    "            return val, ace\n",
    "    \n",
    "    def dealerLogic(self, val, ace):\n",
    "        if val > 21:\n",
    "            if ace:\n",
    "                val -= 10\n",
    "                ace = False\n",
    "            else:\n",
    "                return val, ace, True    \n",
    "\n",
    "        # Assuming dealer stands on hard, hits soft 17, can be changed later\n",
    "        if val > 17 or (val == 17 and not ace):\n",
    "            return val, ace, True\n",
    "        \n",
    "        card = self.giveCard()\n",
    "        if card == 1:\n",
    "            if val + 11 > 21:\n",
    "                # Card must be hard ace (1)\n",
    "                return val + 1, ace, False\n",
    "            else:\n",
    "                # Card can be a soft ace (11)\n",
    "                return val + 11, True, False\n",
    "        else:\n",
    "            return val+card, ace, False\n",
    "    \n",
    "\n",
    "    def chooseAction(self):\n",
    "        # Always hit if val <= 11. Can change for testing\n",
    "        current_val = self.state[0]\n",
    "        if current_val <= 11:\n",
    "            return 1\n",
    "        \n",
    "        # Here we balance experimentation vs explotiation\n",
    "            # This means exp_r precent of the time, hit or stay is chosen at random\n",
    "                # Otherwise, we rely on action that gains the besr reward\n",
    "        \n",
    "        # 'Decide' if a random choice will be made \n",
    "        if np.random.uniform(0, 1) <= self.exp_r:\n",
    "            decision = np.random.choice(self.actions)\n",
    "            \n",
    "        else:\n",
    "            # Greedy action\n",
    "            v = float('-inf')\n",
    "            decision = 0\n",
    "            for a in self.player_Q_Values[self.state]:\n",
    "                if self.player_Q_Values[self.state][a] > v:\n",
    "                    decision = a\n",
    "                    v = self.player_Q_Values[self.state][a]\n",
    "        return decision\n",
    "\n",
    "    # Take in action, update to next state, and determine if game is over\n",
    "    def playerNxtState(self, action):\n",
    "            current_val, up_card, ace = self.state      #unpack tuple\n",
    "            \n",
    "            if action:\n",
    "                card = self.giveCard()\n",
    "                if card == 1:\n",
    "                    if current_val <= 10:\n",
    "                        current_val += 11\n",
    "                        ace = True\n",
    "                    else:\n",
    "                        current_val += 1\n",
    "                else:\n",
    "                    current_val += card\n",
    "            else:\n",
    "                # Player stands\n",
    "                self.end = True\n",
    "            \n",
    "            if current_val > 21:\n",
    "                if ace:\n",
    "                    current_val -= 10\n",
    "                    ace = False\n",
    "                else:\n",
    "                    self.end = True\n",
    "            self.state = (current_val, up_card, ace)\n",
    "\n",
    "        \n",
    "    def reward(self, player_val, dealer_val, end=True):\n",
    "        reward = 0\n",
    "        if end:\n",
    "            if player_val > 21:\n",
    "                reward = -1\n",
    "            else:\n",
    "                if dealer_val > 21 or player_val > dealer_val:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1 if player_val < dealer_val else 0\n",
    "            if not self.training:\n",
    "                return reward\n",
    "\n",
    "        # Backpropagate the determined reward\n",
    "        # Called a Q-value update gradient descent\n",
    "        for s in reversed(self.player_state_action):\n",
    "            state, action = s\n",
    "            reward = self.learn_r * (reward - self.player_Q_Values[state][action]) + self.player_Q_Values[state][action]\n",
    "            self.player_Q_Values[state][action] = round(reward, 3)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_state_action = []\n",
    "        self.state = (0, 0, False)  # initial state\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def play(self, rounds=1000):\n",
    "        for round in range(rounds):\n",
    "            if round % 1000 == 0:  # Check every 1000 rounds\n",
    "                pass\n",
    "                #print(f\"Round: {round}\")\n",
    "\n",
    "            # Deal\n",
    "            dealer_val, d_ace, up_card = self.deal2cards(show=True)\n",
    "            player_val, p_ace = self.deal2cards(show=False)\n",
    "\n",
    "            self.state = (player_val, up_card, p_ace)\n",
    "            #print(\"init\", self.state)\n",
    "\n",
    "            if player_val != 21 and dealer_val != 21:\n",
    "                while not self.end:\n",
    "                    action = self.chooseAction() \n",
    "                    if self.state[0] >= 12:\n",
    "                        state_action_pair = [self.state, action]\n",
    "                        self.player_state_action.append(state_action_pair)\n",
    "                    self.playerNxtState(action)\n",
    "\n",
    "                # Dealer plays\n",
    "                end = False\n",
    "                while not end:\n",
    "                    dealer_val, d_ace, end = self.dealerLogic(dealer_val, d_ace)\n",
    "\n",
    "                # Give reward and update Q value\n",
    "                player_val = self.state[0]\n",
    "                #print(\"player value {} | dealer value {}\".format(player_val, dealer_val))\n",
    "                self.reward(player_val, dealer_val)\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def saveStrategy(self, file = \"strategy\"):\n",
    "        f = open(file, 'wb')\n",
    "        pickle.dump(self.player_Q_Values, f)\n",
    "        f.close()\n",
    "    \n",
    "    def loadStrategy(self, file= \"strategy\"):\n",
    "        f = open(file, 'rb')\n",
    "        self.player_Q_Values = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def playStrategy(self, rounds=1000, file = \"strategy\"):\n",
    "        self.loadStrategy(file)\n",
    "        self.exp_r = 0\n",
    "\n",
    "        result = [0,0,0]  #[win, draw, lose]\n",
    "        for round in range(rounds):\n",
    "\n",
    "            dealer_val, d_ace, show_card = self.deal2cards(show=True)\n",
    "            player_val, p_ace = self.deal2cards(show=False)\n",
    "\n",
    "            self.state = (player_val, show_card, p_ace)\n",
    "\n",
    "            # judge winner after 2 cards\n",
    "            #if player_val == 21 or dealer_val == 21:\n",
    "                #if player_val == dealer_val:\n",
    "                    #result[1] += 1\n",
    "                #elif player_val > dealer_val:\n",
    "                    #result[0] += 1\n",
    "                #else:\n",
    "                    #result[2] += 1\n",
    "\n",
    "            if player_val == 21:\n",
    "                if dealer_val == 21:\n",
    "                    result[1] += 1\n",
    "                else:\n",
    "                    result[0] += 1\n",
    "            elif dealer_val == 21:\n",
    "                result[2] += 1\n",
    "\n",
    "            else:\n",
    "                # player's turn\n",
    "                while not self.end:\n",
    "                    action = self.chooseAction()\n",
    "                    self.playerNxtState(action)\n",
    "                \n",
    "                end = False\n",
    "                while not end:\n",
    "                    dealer_val, d_ace, end = self.dealerLogic(dealer_val, d_ace)\n",
    "                player_val = self.state[0]\n",
    "                # print(\"player value {} | dealer value {}\".format(player_value, dealer_value))\n",
    "                w = self.reward(player_val, dealer_val)\n",
    "                if w == 1:\n",
    "                    result[0] += 1\n",
    "                elif w == 0:\n",
    "                    result[1] += 1\n",
    "                else:\n",
    "                    result[2] += 1\n",
    "            self.reset()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayStrategy(strat):  \n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "    # Prepare headers for the tables\n",
    "    headers = \"    A  \" + \"  \".join(f\"{i}\" for i in range(2, 11))\n",
    "    \n",
    "    # Prepare the rows for both Hard and Soft hands\n",
    "    hard_rows = []\n",
    "    soft_rows = []\n",
    "    print(\"            Hard Hands                           Soft Hands\")\n",
    "    print(headers + \"   \" + headers)  # Print headers side by side\n",
    "    \n",
    "    for i in range(12, 22):\n",
    "        hard_row = f\"{i:<2}  \"\n",
    "        soft_row = f\"{i:<2}  \"\n",
    "        for j in range(1, 11):\n",
    "            # Hard hands row\n",
    "            if strat[(i, j, False)][1] > strat[(i, j, False)][0]:\n",
    "                hard_action = GREEN + 'H'\n",
    "            else:\n",
    "                hard_action = RED + 'S'  # Red for 'S'\n",
    "            hard_row += f\"{hard_action + RESET}  \"\n",
    "            \n",
    "            # Soft hands row\n",
    "            if strat[(i, j, True)][1] > strat[(i, j, True)][0]:\n",
    "                soft_action = GREEN + 'H' \n",
    "            else:\n",
    "                soft_action = RED + 'S'  # Red for 'S'\n",
    "            soft_row += f\"{soft_action + RESET}  \"\n",
    "\n",
    "        # Append each row to their respective list\n",
    "        hard_rows.append(hard_row)\n",
    "        soft_rows.append(soft_row)\n",
    "    \n",
    "    # Print each row side by side\n",
    "    for hard_row, soft_row in zip(hard_rows, soft_rows):\n",
    "        print(hard_row + \"  \" + soft_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "### TRAIN ###\n",
    "\n",
    "learn_r = 0.2\n",
    "exp_r = 0.1\n",
    "rounds = 5000000\n",
    "\n",
    "\n",
    "bot = Qlearning(learn_r, exp_r)\n",
    "bot.play(rounds)\n",
    "print(\"Training Complete\")\n",
    "bot.saveStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookBot:\n",
      "[W-T-L]: [4362, 891, 4747]\n",
      "Win rate: 43.62%\n",
      "\n",
      "TrainedBot:\n",
      "[W-T-L]: [4322, 784, 4894]\n",
      "Win rate: 43.22%\n"
     ]
    }
   ],
   "source": [
    "### Compare Gameplay Between Book and Trained Strategies ###\n",
    "\n",
    "games = 10000\n",
    "\n",
    "bookBot = Qlearning(train=False)\n",
    "book_score = bookBot.playStrategy(games,\"book_strategy\")\n",
    "\n",
    "trainedBot = Qlearning(train=False)\n",
    "trained_score = bookBot.playStrategy(games,\"strategy\")\n",
    "\n",
    "\n",
    "print(f\"BookBot:\\n[W-T-L]: {book_score}\\nWin rate: {round((book_score[0]/games)*100,3)}%\\n\")\n",
    "print(f\"TrainedBot:\\n[W-T-L]: {trained_score}\\nWin rate: {round((trained_score[0]/games)*100,3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t    Book Strategy\n",
      "\n",
      "            Hard Hands                           Soft Hands\n",
      "    A  2  3  4  5  6  7  8  9  10       A  2  3  4  5  6  7  8  9  10\n",
      "12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "13  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    13  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    14  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "15  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    15  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "16  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    16  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "17  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    17  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "18  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    18  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "\n",
      "\t\t\t    Current Strategy\n",
      "\n",
      "            Hard Hands                           Soft Hands\n",
      "    A  2  3  4  5  6  7  8  9  10       A  2  3  4  5  6  7  8  9  10\n",
      "12  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "13  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    13  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "15  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m    15  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "16  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m    16  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "17  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    17  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "18  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    18  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    19  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n"
     ]
    }
   ],
   "source": [
    "### Compare Table of Trained Strategy to Book Strategy ###\n",
    "\n",
    "bookStrat = Qlearning()\n",
    "bookStrat.loadStrategy(\"book_strategy\")\n",
    "\n",
    "compareStrat = Qlearning()\n",
    "compareStrat.loadStrategy(\"strategy\")\n",
    "\n",
    "print(\"\\t\\t\\t    Book Strategy\\n\")\n",
    "displayStrategy(bookStrat.player_Q_Values)\n",
    "print(\"\\n\\t\\t\\t    Current Strategy\\n\")\n",
    "displayStrategy(compareStrat.player_Q_Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top three performances\n",
    "* Learn Rate: 0.09, Exp Rate: 0.19, Avg Win Rate: 43.39333333333334%\n",
    " * Learn Rate: 0.01, Exp Rate: 0.23, Avg Win Rate: 43.339999999999996%\n",
    " * Learn Rate: 0.06, Exp Rate: 0.9, Avg Win Rate: 43.333333333333336%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing learn_r=0.01, exp_r=0.05\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.05. Trained Avg: 42.093333333333334%\n",
      "Testing learn_r=0.01, exp_r=0.06\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.06. Trained Avg: 42.46666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.07\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.07. Trained Avg: 42.06666666666666%\n",
      "Testing learn_r=0.01, exp_r=0.08\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.08. Trained Avg: 42.36000000000001%\n",
      "Testing learn_r=0.01, exp_r=0.09\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.09. Trained Avg: 41.75333333333333%\n",
      "Testing learn_r=0.01, exp_r=0.1\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.1. Trained Avg: 41.75333333333334%\n",
      "Testing learn_r=0.01, exp_r=0.11\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.11. Trained Avg: 41.653333333333336%\n",
      "Testing learn_r=0.01, exp_r=0.12\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.12. Trained Avg: 42.160000000000004%\n",
      "Testing learn_r=0.01, exp_r=0.13\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.13. Trained Avg: 41.986666666666665%\n",
      "Testing learn_r=0.01, exp_r=0.14\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.14. Trained Avg: 41.42666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.15\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.15. Trained Avg: 42.00666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.16\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.16. Trained Avg: 42.53333333333333%\n",
      "Testing learn_r=0.01, exp_r=0.17\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.17. Trained Avg: 41.946666666666665%\n",
      "Testing learn_r=0.01, exp_r=0.18\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.18. Trained Avg: 42.300000000000004%\n",
      "Testing learn_r=0.01, exp_r=0.19\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.19. Trained Avg: 42.70666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.2\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.2. Trained Avg: 42.54%\n",
      "Testing learn_r=0.01, exp_r=0.21\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.21. Trained Avg: 42.01333333333333%\n",
      "Testing learn_r=0.01, exp_r=0.22\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.22. Trained Avg: 41.653333333333336%\n",
      "Testing learn_r=0.01, exp_r=0.23\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.23. Trained Avg: 42.04666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.24\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.24. Trained Avg: 42.42666666666667%\n",
      "Testing learn_r=0.01, exp_r=0.25\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.25. Trained Avg: 42.52%\n",
      "Testing learn_r=0.01, exp_r=0.5\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.5. Trained Avg: 41.953333333333326%\n",
      "Testing learn_r=0.01, exp_r=0.9\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.01, exp_r=0.9. Trained Avg: 43.153333333333336%\n",
      "Testing learn_r=0.02, exp_r=0.05\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.05. Trained Avg: 42.1%\n",
      "Testing learn_r=0.02, exp_r=0.06\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.06. Trained Avg: 42.0%\n",
      "Testing learn_r=0.02, exp_r=0.07\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.07. Trained Avg: 42.419999999999995%\n",
      "Testing learn_r=0.02, exp_r=0.08\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.08. Trained Avg: 42.1%\n",
      "Testing learn_r=0.02, exp_r=0.09\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.09. Trained Avg: 41.36666666666667%\n",
      "Testing learn_r=0.02, exp_r=0.1\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.1. Trained Avg: 42.160000000000004%\n",
      "Testing learn_r=0.02, exp_r=0.11\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.11. Trained Avg: 42.95333333333334%\n",
      "Testing learn_r=0.02, exp_r=0.12\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.12. Trained Avg: 41.379999999999995%\n",
      "Testing learn_r=0.02, exp_r=0.13\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.13. Trained Avg: 42.653333333333336%\n",
      "Testing learn_r=0.02, exp_r=0.14\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.14. Trained Avg: 42.22%\n",
      "Testing learn_r=0.02, exp_r=0.15\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.15. Trained Avg: 43.106666666666676%\n",
      "Testing learn_r=0.02, exp_r=0.16\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.16. Trained Avg: 41.98%\n",
      "Testing learn_r=0.02, exp_r=0.17\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.17. Trained Avg: 42.18666666666667%\n",
      "Testing learn_r=0.02, exp_r=0.18\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.18. Trained Avg: 41.74666666666667%\n",
      "Testing learn_r=0.02, exp_r=0.19\n",
      " Trial 1/3\n",
      " Trial 2/3\n",
      " Trial 3/3\n",
      "Finished testing for learn_r=0.02, exp_r=0.19. Trained Avg: 42.73333333333333%\n",
      "Testing learn_r=0.02, exp_r=0.2\n",
      " Trial 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[266], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m learn_rates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;241m0.03\u001b[39m, \u001b[38;5;241m0.04\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.06\u001b[39m, \u001b[38;5;241m0.07\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.11\u001b[39m, \u001b[38;5;241m0.12\u001b[39m, \u001b[38;5;241m0.13\u001b[39m, \u001b[38;5;241m0.14\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.18\u001b[39m, \u001b[38;5;241m0.19\u001b[39m, \u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.00001\u001b[39m]\n\u001b[1;32m     57\u001b[0m exp_rates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.06\u001b[39m, \u001b[38;5;241m0.07\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.11\u001b[39m, \u001b[38;5;241m0.12\u001b[39m, \u001b[38;5;241m0.13\u001b[39m, \u001b[38;5;241m0.14\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.16\u001b[39m, \u001b[38;5;241m0.17\u001b[39m, \u001b[38;5;241m0.18\u001b[39m, \u001b[38;5;241m0.19\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.21\u001b[39m, \u001b[38;5;241m0.22\u001b[39m, \u001b[38;5;241m0.23\u001b[39m, \u001b[38;5;241m0.24\u001b[39m, \u001b[38;5;241m0.25\u001b[39m,\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.9\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m top_three \u001b[38;5;241m=\u001b[39m \u001b[43mtest_learning_rates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[266], line 19\u001b[0m, in \u001b[0;36mtest_learning_rates\u001b[0;34m(learn_rates, exp_rates, trials, training_rounds, test_games)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the bot\u001b[39;00m\n\u001b[1;32m     18\u001b[0m bot \u001b[38;5;241m=\u001b[39m Qlearning(learn_r\u001b[38;5;241m=\u001b[39mlr, exp_r\u001b[38;5;241m=\u001b[39mer)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m bot\u001b[38;5;241m.\u001b[39msaveStrategy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Test the trained bot\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[261], line 170\u001b[0m, in \u001b[0;36mQlearning.play\u001b[0;34m(self, rounds)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m player_val \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m21\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dealer_val \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m21\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend:\n\u001b[0;32m--> 170\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m:\n\u001b[1;32m    172\u001b[0m             state_action_pair \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, action]\n",
      "Cell \u001b[0;32mIn[261], line 88\u001b[0m, in \u001b[0;36mQlearning.chooseAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Here we balance experimentation vs explotiation\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# This means exp_r precent of the time, hit or stay is chosen at random\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# Otherwise, we rely on action that gains the besr reward\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# 'Decide' if a random choice will be made \u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_r:\n\u001b[1;32m     89\u001b[0m     decision \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Chat GPT generated code. Goal is to find the best learn/ exp rate combination ###\n",
    "\n",
    "def test_learning_rates(learn_rates, exp_rates, trials=10, training_rounds=20000, test_games=5000):\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    performance_list = []\n",
    "\n",
    "    # Loop through each combination of learning and experimenting rates\n",
    "    for lr, er in product(learn_rates, exp_rates):\n",
    "        avg_trained_wins = []\n",
    "        print(f\"Testing learn_r={lr}, exp_r={er}\")\n",
    "\n",
    "        # Perform trials\n",
    "        for trial in range(trials):\n",
    "            print(f\" Trial {trial+1}/{trials}\")\n",
    "\n",
    "            # Train the bot\n",
    "            bot = Qlearning(learn_r=lr, exp_r=er)\n",
    "            bot.play(training_rounds)\n",
    "            bot.saveStrategy(\"temp_strategy\")\n",
    "\n",
    "            # Test the trained bot\n",
    "            trainedBot = Qlearning(train=False)\n",
    "            trained_score = trainedBot.playStrategy(test_games, \"temp_strategy\")\n",
    "\n",
    "            # Calculate win rates\n",
    "            trained_win_rate = (trained_score[0] / test_games) * 100\n",
    "\n",
    "            # Store results for this trial\n",
    "            avg_trained_wins.append(trained_win_rate)\n",
    "\n",
    "        # Compute average win rate over all trials for the current parameter combination\n",
    "        average_trained = np.mean(avg_trained_wins)\n",
    "\n",
    "        # Store in results dictionary\n",
    "        results[(lr, er)] = average_trained\n",
    "\n",
    "        # Add to the list for sorting and determining top performances\n",
    "        performance_list.append((average_trained, lr, er))\n",
    "\n",
    "        # Output intermediate results\n",
    "        print(f\"Finished testing for learn_r={lr}, exp_r={er}. Trained Avg: {average_trained}%\")\n",
    "\n",
    "    # Sort performances and select the top three\n",
    "    top_performances = sorted(performance_list, reverse=True, key=lambda x: x[0])[:3]\n",
    "\n",
    "    # Print the top three performances\n",
    "    print(\"Top three performances:\")\n",
    "    for performance in top_performances:\n",
    "        win_rate, lr, er = performance\n",
    "        print(f\"Learn Rate: {lr}, Exp Rate: {er}, Avg Win Rate: {win_rate}%\")\n",
    "\n",
    "    return top_performances\n",
    "\n",
    "# Example usage:\n",
    "learn_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2,0.00001]\n",
    "exp_rates = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25,0.5,0.9]\n",
    "top_three = test_learning_rates(learn_rates, exp_rates, trials=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bjbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
