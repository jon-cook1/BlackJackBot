{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    # learn_r -> learning rate      exp_r -> experimenting rate (how often to randomly choose)\n",
    "    def __init__(self, learn_r=0.1, exp_r=0.2, train=True, gamma = 0.9):  \n",
    "\n",
    "        self.player_Q_Values = {}  \n",
    "        \n",
    "        # key: [(player_val, up_card, ace)][action] = value\n",
    "        # initialise Q values | (12-21) x (1-10) x (True, False) x (1, 0) 400 in total\n",
    "            # This essentially represents each possible action in the game. Each combination of\n",
    "                # player sum (hard and soft)/ dealer up card with 1 and 0 to represent the choice for that combination\n",
    "        \n",
    "        for i in range(12, 22):\n",
    "            for j in range(1, 11):\n",
    "                for k in [True, False]:\n",
    "                    self.player_Q_Values[(i, j, k)] = {}\n",
    "                    for a in [1, 0]:\n",
    "                        if i == 21 and a == 0:\n",
    "                            self.player_Q_Values[(i, j, k)][a] = 1\n",
    "                        else:\n",
    "                            self.player_Q_Values[(i, j, k)][a] = 0\n",
    "        \n",
    "        \n",
    "        self.state = (0, 0, False)  # default state\n",
    "        self.actions = [1, 0]  # 1 -> hit, 0 -> stand\n",
    "        self.player_state_action = []\n",
    "        self.end = False\n",
    "        self.learn_r = learn_r\n",
    "        self.exp_r = exp_r\n",
    "        self.training = train\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Same logic here as in MC\n",
    "    @staticmethod\n",
    "    def giveCard():\n",
    "        cards = [1,2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "        return np.random.choice(cards)\n",
    "    \n",
    "    # Used to allow auto win if 21 is pulled off deal\n",
    "    def deal2cards(self, show=False):\n",
    "        cards = [self.giveCard(), self.giveCard()]\n",
    "        \n",
    "        val = sum(cards)\n",
    "        if 1 in cards:\n",
    "            val += 10\n",
    "            ace = True\n",
    "        else:\n",
    "            ace = False\n",
    "\n",
    "        if show:\n",
    "            return val, ace, cards[0]\n",
    "        else:\n",
    "            return val, ace\n",
    "    \n",
    "    def dealerLogic(self, val, ace):\n",
    "        if val > 21:\n",
    "            if ace:\n",
    "                val -= 10\n",
    "                ace = False\n",
    "            else:\n",
    "                return val, ace, True    \n",
    "\n",
    "        # Assuming dealer stands on hard, hits soft 17, can be changed later\n",
    "        if val > 17 or (val == 17 and not ace):\n",
    "            return val, ace, True\n",
    "        \n",
    "        card = self.giveCard()\n",
    "        if card == 1:\n",
    "            if val + 11 > 21:\n",
    "                # Card must be hard ace (1)\n",
    "                return val + 1, ace, False\n",
    "            else:\n",
    "                # Card can be a soft ace (11)\n",
    "                return val + 11, True, False\n",
    "        else:\n",
    "            return val+card, ace, False\n",
    "    \n",
    "\n",
    "    def chooseAction(self):\n",
    "        # Always hit if val <= 11. Can change for testing\n",
    "        current_val = self.state[0]\n",
    "        if current_val <= 11:\n",
    "            return 1\n",
    "        \n",
    "        # Here we balance experimentation vs explotiation\n",
    "            # This means exp_r precent of the time, hit or stay is chosen at random\n",
    "                # Otherwise, we rely on action that gains the besr reward\n",
    "        \n",
    "        # 'Decide' if a random choice will be made \n",
    "        if self.training and np.random.uniform(0, 1) <= self.exp_r:\n",
    "            decision = np.random.choice(self.actions)\n",
    "\n",
    "        else:\n",
    "            # Greedy action\n",
    "            v = float('-inf')\n",
    "            decision = 0\n",
    "            for a in self.player_Q_Values[self.state]:\n",
    "                if self.player_Q_Values[self.state][a] > v:\n",
    "                    decision = a\n",
    "                    v = self.player_Q_Values[self.state][a]\n",
    "        return decision\n",
    "\n",
    "    # Take in action, update to next state, and determine if game is over\n",
    "    def playerNxtState(self, action):\n",
    "            current_val, up_card, ace = self.state      #unpack tuple\n",
    "            \n",
    "            if action:\n",
    "                card = self.giveCard()\n",
    "                if card == 1:\n",
    "                    if current_val <= 10:\n",
    "                        current_val += 11\n",
    "                        ace = True\n",
    "                    else:\n",
    "                        current_val += 1\n",
    "                else:\n",
    "                    current_val += card\n",
    "            else:\n",
    "                # Player stands\n",
    "                self.end = True\n",
    "            \n",
    "            if current_val > 21:\n",
    "                if ace:\n",
    "                    current_val -= 10\n",
    "                    ace = False\n",
    "                else:\n",
    "                    self.end = True\n",
    "            self.state = (current_val, up_card, ace)\n",
    "\n",
    "        \n",
    "    def reward(self, player_val, dealer_val, end=True):\n",
    "        reward = 0\n",
    "        if end:\n",
    "            if player_val > 21:\n",
    "                reward = -1\n",
    "            else:\n",
    "                if dealer_val > 21 or player_val > dealer_val:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1 if player_val < dealer_val else 0\n",
    "        \n",
    "        gamma = self.gamma  # Discount factor\n",
    "        if not self.training:\n",
    "            return reward\n",
    "\n",
    "        # Backpropagate the determined reward\n",
    "        # Called a Q-value update gradient descent\n",
    "          # Backpropagate the determined reward\n",
    "        # Called a Q-value update gradient descent\n",
    "        for s in reversed(self.player_state_action):\n",
    "            state, action = s\n",
    "            reward = self.learn_r * (reward - self.player_Q_Values[state][action]) + self.player_Q_Values[state][action]\n",
    "            self.player_Q_Values[state][action] = round(reward, 3)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_state_action = []\n",
    "        self.state = (0, 0, False)  # initial state\n",
    "        self.end = False\n",
    "\n",
    "\n",
    "    def play(self, rounds=1000):\n",
    "        for round in range(rounds):\n",
    "            if round % 1000 == 0:  # Check every 1000 rounds\n",
    "                pass\n",
    "                #print(f\"Round: {round}\")\n",
    "\n",
    "            # Deal\n",
    "            dealer_val, d_ace, up_card = self.deal2cards(show=True)\n",
    "            player_val, p_ace = self.deal2cards(show=False)\n",
    "\n",
    "            self.state = (player_val, up_card, p_ace)\n",
    "            #print(\"init\", self.state)\n",
    "\n",
    "            if player_val != 21 and dealer_val != 21:\n",
    "                while not self.end:\n",
    "                    action = self.chooseAction() \n",
    "                    if self.state[0] >= 12:\n",
    "                        state_action_pair = [self.state, action]\n",
    "                        self.player_state_action.append(state_action_pair)\n",
    "                    self.playerNxtState(action)\n",
    "\n",
    "                # Dealer plays\n",
    "                end = False\n",
    "                while not end:\n",
    "                    dealer_val, d_ace, end = self.dealerLogic(dealer_val, d_ace)\n",
    "\n",
    "                # Give reward and update Q value\n",
    "                player_val = self.state[0]\n",
    "                #print(\"player value {} | dealer value {}\".format(player_val, dealer_val))\n",
    "                self.reward(player_val, dealer_val)\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def saveStrategy(self, file = \"strategy\"):\n",
    "        f = open(file, 'wb')\n",
    "        pickle.dump(self.player_Q_Values, f)\n",
    "        f.close()\n",
    "    \n",
    "    def loadStrategy(self, file= \"strategy\"):\n",
    "        f = open(file, 'rb')\n",
    "        self.player_Q_Values = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def playStrategy(self, rounds=1000, file = \"strategy\"):\n",
    "        self.loadStrategy(file)\n",
    "        self.exp_r = 0\n",
    "\n",
    "        result = [0,0,0]  #[win, draw, lose]\n",
    "        for round in range(rounds):\n",
    "\n",
    "            dealer_val, d_ace, show_card = self.deal2cards(show=True)\n",
    "            player_val, p_ace = self.deal2cards(show=False)\n",
    "\n",
    "            self.state = (player_val, show_card, p_ace)\n",
    "\n",
    "            # judge winner after 2 cards\n",
    "            #if player_val == 21 or dealer_val == 21:\n",
    "                #if player_val == dealer_val:\n",
    "                    #result[1] += 1\n",
    "                #elif player_val > dealer_val:\n",
    "                    #result[0] += 1\n",
    "                #else:\n",
    "                    #result[2] += 1\n",
    "\n",
    "            if player_val == 21:\n",
    "                if dealer_val == 21:\n",
    "                    result[1] += 1\n",
    "                else:\n",
    "                    result[0] += 1\n",
    "            elif dealer_val == 21:\n",
    "                result[2] += 1\n",
    "\n",
    "            else:\n",
    "                # player's turn\n",
    "                while not self.end:\n",
    "                    action = self.chooseAction()\n",
    "                    self.playerNxtState(action)\n",
    "                \n",
    "                end = False\n",
    "                while not end:\n",
    "                    dealer_val, d_ace, end = self.dealerLogic(dealer_val, d_ace)\n",
    "                player_val = self.state[0]\n",
    "                # print(\"player value {} | dealer value {}\".format(player_value, dealer_value))\n",
    "                w = self.reward(player_val, dealer_val)\n",
    "                if w == 1:\n",
    "                    result[0] += 1\n",
    "                elif w == 0:\n",
    "                    result[1] += 1\n",
    "                else:\n",
    "                    result[2] += 1\n",
    "            self.reset()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayStrategy(strat):  \n",
    "    RED = '\\033[91m'\n",
    "    GREEN = '\\033[92m'\n",
    "    RESET = '\\033[0m'\n",
    "\n",
    "    # Prepare headers for the tables\n",
    "    headers = \"    A  \" + \"  \".join(f\"{i}\" for i in range(2, 11))\n",
    "    \n",
    "    # Prepare the rows for both Hard and Soft hands\n",
    "    hard_rows = []\n",
    "    soft_rows = []\n",
    "    print(\"            Hard Hands                           Soft Hands\")\n",
    "    print(headers + \"   \" + headers)  # Print headers side by side\n",
    "    \n",
    "    for i in range(12, 22):\n",
    "        hard_row = f\"{i:<2}  \"\n",
    "        soft_row = f\"{i:<2}  \"\n",
    "        for j in range(1, 11):\n",
    "            # Hard hands row\n",
    "            if strat[(i, j, False)][1] > strat[(i, j, False)][0]:\n",
    "                hard_action = GREEN + 'H'\n",
    "            else:\n",
    "                hard_action = RED + 'S'  # Red for 'S'\n",
    "            hard_row += f\"{hard_action + RESET}  \"\n",
    "            \n",
    "            # Soft hands row\n",
    "            if strat[(i, j, True)][1] > strat[(i, j, True)][0]:\n",
    "                soft_action = GREEN + 'H' \n",
    "            else:\n",
    "                soft_action = RED + 'S'  # Red for 'S'\n",
    "            soft_row += f\"{soft_action + RESET}  \"\n",
    "\n",
    "        # Append each row to their respective list\n",
    "        hard_rows.append(hard_row)\n",
    "        soft_rows.append(soft_row)\n",
    "    \n",
    "    # Print each row side by side\n",
    "    for hard_row, soft_row in zip(hard_rows, soft_rows):\n",
    "        print(hard_row + \"  \" + soft_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN ###\n",
    "\n",
    "learn_r = 0.1\n",
    "exp_r = 0.2\n",
    "gamma_r = 0.9\n",
    "\n",
    "rounds = 100000\n",
    "\n",
    "bot = Qlearning(learn_r, exp_r,True,gamma_r)\n",
    "bot.play(rounds)\n",
    "print(\"Training Complete\")\n",
    "bot.saveStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookBot:\n",
      "[W-T-L]: [8641884, 1741956, 9616160]\n",
      "Win rate: 43.209%\n",
      "\n",
      "Not Losing Rate: 51.919\n",
      "\n",
      "TrainedBot:\n",
      "[W-T-L]: [8639117, 1740740, 9620143]\n",
      "Win rate: 43.196%\n",
      "Not Losing Rate: 51.899\n"
     ]
    }
   ],
   "source": [
    "### Compare Gameplay Between Book and Trained Strategies ###\n",
    "\n",
    "games = 20000000\n",
    "\n",
    "bookBot = Qlearning(train=False)\n",
    "book_score = bookBot.playStrategy(games,\"book_strategy\")\n",
    "\n",
    "trainedBot = Qlearning(train=False)\n",
    "trained_score = bookBot.playStrategy(games,\"averaged_strategy\")\n",
    "\n",
    "\n",
    "print(f\"BookBot:\\n[W-T-L]: {book_score}\\nWin rate: {round((book_score[0]/games)*100,3)}%\\n\")\n",
    "print(\"Not Losing Rate:\",round((book_score[0]+book_score[1])/games*100,3))\n",
    "print(f\"\\nTrainedBot:\\n[W-T-L]: {trained_score}\\nWin rate: {round((trained_score[0]/games)*100,3)}%\")\n",
    "print(\"Not Losing Rate:\",round((trained_score[0]+trained_score[1])/games*100,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t    Book Strategy\n",
      "\n",
      "            Hard Hands                           Soft Hands\n",
      "    A  2  3  4  5  6  7  8  9  10       A  2  3  4  5  6  7  8  9  10\n",
      "12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "13  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    13  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    14  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "15  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    15  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "16  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    16  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "17  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    17  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "18  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    18  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "\n",
      "\t\t\t    Trained Strategy\n",
      "\n",
      "            Hard Hands                           Soft Hands\n",
      "    A  2  3  4  5  6  7  8  9  10       A  2  3  4  5  6  7  8  9  10\n",
      "12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "13  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    13  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    14  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "15  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    15  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "16  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    16  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "17  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    17  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "18  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    18  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n"
     ]
    }
   ],
   "source": [
    "### Compare Table of Trained Strategy to Book Strategy ###\n",
    "\n",
    "bookStrat = Qlearning()\n",
    "bookStrat.loadStrategy(\"book_strategy\")\n",
    "\n",
    "compareStrat = Qlearning()\n",
    "compareStrat.loadStrategy(\"averaged_strategy\")\n",
    "\n",
    "print(\"\\t\\t\\t    Book Strategy\\n\")\n",
    "displayStrategy(bookStrat.player_Q_Values)\n",
    "print(\"\\n\\t\\t\\t    Trained Strategy\\n\")\n",
    "displayStrategy(compareStrat.player_Q_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0\n",
      "Round: 500\n",
      "            Hard Hands                           Soft Hands\n",
      "    A  2  3  4  5  6  7  8  9  10       A  2  3  4  5  6  7  8  9  10\n",
      "12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    12  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "13  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    13  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "14  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    14  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "15  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    15  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "16  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m    16  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "17  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    17  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "18  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    18  \u001b[92mH\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[92mH\u001b[0m  \u001b[92mH\u001b[0m  \n",
      "19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    19  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    20  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n",
      "21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m    21  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \u001b[91mS\u001b[0m  \n"
     ]
    }
   ],
   "source": [
    "# Train x amount of times and average together\n",
    "cohorts = 1000\n",
    "learn_r = 0.09\n",
    "exp_r = 0.23\n",
    "gamma_r = 0.8 #not being used right now\n",
    "rounds = 50000\n",
    "\n",
    "#1000, 0.09, 0.23, 50000 resulted in 100% accuracy. 22 min to train.\n",
    "\n",
    "mainstrat = Qlearning()\n",
    "mainstrat.loadStrategy(\"empty_strategy\")\n",
    "\n",
    "\n",
    "for i in range(cohorts):\n",
    "    if i % 500 == 0:\n",
    "        print(\"Round:\",i)\n",
    "    bot = Qlearning(learn_r, exp_r,True,gamma_r)\n",
    "    bot.play(rounds)\n",
    "\n",
    "    for j in bot.player_Q_Values:\n",
    "        '''\n",
    "        if bot.player_Q_Values[j][0] > bot.player_Q_Values[j][1]:\n",
    "            mainstrat.player_Q_Values[j][0] += 1\n",
    "        else:\n",
    "            mainstrat.player_Q_Values[j][1] += 1\n",
    "        '''\n",
    "        \n",
    "        mainstrat.player_Q_Values[j][0] += bot.player_Q_Values[j][0]\n",
    "        mainstrat.player_Q_Values[j][1] += bot.player_Q_Values[j][1]\n",
    "\n",
    "displayStrategy(mainstrat.player_Q_Values)\n",
    "mainstrat.saveStrategy(\"averaged_strategy1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top three performances\n",
    "* Learn Rate: 0.09, Exp Rate: 0.19, Avg Win Rate: 43.39333333333334%\n",
    " * Learn Rate: 0.01, Exp Rate: 0.23, Avg Win Rate: 43.339999999999996%\n",
    " * Learn Rate: 0.06, Exp Rate: 0.9, Avg Win Rate: 43.333333333333336%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chat GPT generated code. Goal is to find the best learn/ exp rate combination ###\n",
    "\n",
    "def test_learning_rates(learn_rates, exp_rates, trials=10, training_rounds=20000, test_games=5000):\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    performance_list = []\n",
    "\n",
    "    # Loop through each combination of learning and experimenting rates\n",
    "    for lr, er in product(learn_rates, exp_rates):\n",
    "        avg_trained_wins = []\n",
    "        print(f\"Testing learn_r={lr}, exp_r={er}\")\n",
    "\n",
    "        # Perform trials\n",
    "        for trial in range(trials):\n",
    "            print(f\" Trial {trial+1}/{trials}\")\n",
    "\n",
    "            # Train the bot\n",
    "            bot = Qlearning(learn_r=lr, exp_r=er)\n",
    "            bot.play(training_rounds)\n",
    "            bot.saveStrategy(\"temp_strategy\")\n",
    "\n",
    "            # Test the trained bot\n",
    "            trainedBot = Qlearning(train=False)\n",
    "            trained_score = trainedBot.playStrategy(test_games, \"temp_strategy\")\n",
    "\n",
    "            # Calculate win rates\n",
    "            trained_win_rate = (trained_score[0] / test_games) * 100\n",
    "\n",
    "            # Store results for this trial\n",
    "            avg_trained_wins.append(trained_win_rate)\n",
    "\n",
    "        # Compute average win rate over all trials for the current parameter combination\n",
    "        average_trained = np.mean(avg_trained_wins)\n",
    "\n",
    "        # Store in results dictionary\n",
    "        results[(lr, er)] = average_trained\n",
    "\n",
    "        # Add to the list for sorting and determining top performances\n",
    "        performance_list.append((average_trained, lr, er))\n",
    "\n",
    "        # Output intermediate results\n",
    "        print(f\"Finished testing for learn_r={lr}, exp_r={er}. Trained Avg: {average_trained}%\")\n",
    "\n",
    "    # Sort performances and select the top three\n",
    "    top_performances = sorted(performance_list, reverse=True, key=lambda x: x[0])[:3]\n",
    "\n",
    "    # Print the top three performances\n",
    "    print(\"Top three performances:\")\n",
    "    for performance in top_performances:\n",
    "        win_rate, lr, er = performance\n",
    "        print(f\"Learn Rate: {lr}, Exp Rate: {er}, Avg Win Rate: {win_rate}%\")\n",
    "\n",
    "    return top_performances\n",
    "\n",
    "# Example usage:\n",
    "learn_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2,0.00001]\n",
    "exp_rates = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25,0.5,0.9]\n",
    "#top_three = test_learning_rates(learn_rates, exp_rates, trials=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bjbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
